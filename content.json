{"posts":[{"title":"Procella: Unifying serving and analytical data at Youtube","text":"Procella是Youtube新一代的SQL处理引擎 [1]，其能够支持批式和实时的数据导入，并为不同的分析场景提供统一和高效的数据查询服务。 Youtube内部的数据分析场景主要分为报表和大盘、内嵌统计、时序监控和Ad-hoc查询等四类。这四类查询在数据量、查询复杂度和查询吞吐上具有完全不同的特点。例如报表和大盘需要处理的数据量较大，查询更加复杂，但对数据的新鲜度的要求较低；而内嵌统计和时序监控则要求最高的数据新鲜度，其需要处理不断变化的数据，并支持每秒数百万次的查询。 在过去，这四类数据分析场景分别由不同的系统来支持，例如Adhoc分析和内部小数据的报表需求由Dremel来支持；面向外部的大数据报表由Mesa和Bigtable来支持；监控由Monarch支持；而嵌入统计由Vitess来支持。但对不同的场景使用不同的系统来支持，存在着以下的挑战 数据需要在不同的系统之间流转，带来了很多不必要的计算和存储资源的浪费。 在不同系统中也很难维护数据的一致性。 由于这些系统的API和语义并不完全相同，因此对于开发人员来说，使用这些不同系统的学习成本较高，开发效率也较低。 为了解决上述问题，Youtube研发了Procella，来为不同的数据分析场景提供统一的数据导入和查询服务。 用户接口和数据库类似，Procella中的数据被组织成一系列的表。这些表的数据会被保存在多个文件中，其中每个文件被称为一个tablet或一个partition。Procella支持多层的分区（partitioning）和聚簇（clustering）。一般事实表会按照日期来分区，而在分区内数据会按照多个维度进行聚簇。而维度表则一般会按照维度进行划分和排序。 Procella通过标准的DDL语句来对表进行创建、删除和修改。用户可以在表的定义中指定和修改列的名称和数据类型、表的划分和排序方式、约束、和数据导入的方式。对于实时表，用户还可以指定数据的降采样（down-sampling）和过期方式等。而在对数据进行查询时，Procella支持几乎全部的SQL特性，包括多级连接、集合操作和嵌套查询等。 系统架构数据存储Procella使用Google内部的分布式文件系统Colossus来存储数据。 Procella支持多种数据存储格式。早期Procella使用Capacitor来作为主要的数据存储格式，但Capacitor主要是针对Adhoc分析场景进行设计，其能够支持高效的数据扫描，但缺少对数据索引的支持。为了更好的支持内嵌统计和监控等场景，Procella设计了Artus格式，其能够同时支持高效的数据扫描和检索。 Artus没有使用LZW或ZSTD这样通用的压缩算法来对数据进行编码。相反的，Artus使用了一个自适应编码方式。在编码时，Arthus会扫描数据多遍。在第一遍时，会采集数据的一些信息，包括数据的基数、最大值、最小值和排列顺序等。之后Artus会利用这些信息来选择数据的最优编码方式。Artus相比于ZSTD这类通用算法能够获得高达2倍的压缩率。 Artus对嵌套和重复的字段类型也是用了新的编码方式来提高检索效率。 另外，Artus的一个最大的优点是，其可以在不解压数据的情况下执行按主键检索等很多操作。这可以显著的减少在数据查询时的IO开销，提高执行效率。 非常遗憾的是，Procella的论文并没有给出Artus的实现细节。 元数据存储Procella使用BigTable和Spanner来存储表字段信息等元数据信息。 除了表的字段信息，元数据信息还包括了一些用户提高查询效率的统计信息和辅助结构，如数据分区的zonemap，bitmap，倒排索引和bloom filter等。这些元数据一些是在导入或者维护过程中从数据文件的头部中抽取出来的。 计算节点Procella中包含了多种不同角色的计算节点，包括 Root Server（RS）：根节点，用来接收用户查询，对用户查询进行解析，使用MDS提供的元数据信息来对用户查询进行优化，并负责用户查询的调度和执行。 Metadata Server（MDS）：元数据节点，用于支持元数据的查询。 Data Server（DS）：负责执行数据的检索和查询。每个DS会负责一部分数据；而为了提高可靠性，任意一份数据都可能由多个DS提供服务。 Ingestion Server：负责将用户实时导入的数据写入到Procella中。 Compaction Server：负责将导入的数据进行整理合并，形成更适合于查询的文件。 Registration Server：负责对系统中新生成的文件进行校验和登记。这些文件可能是批式导入的文件，也可能是Compaction生成的文件。 所有这些计算节点都运行在Google内部的资源管理系统Borg上。 数据导入数据的导入分为离线导入和实时导入两种方式。 离线导入在离线导入时，用户只需要将需要导入的文件通过RegistrationServer登记到系统中即可。RegistrationServer会对文件的字段信息等进行校验，并将文件的元数据信息（如bloom filter）读取并写入到元数据存储中。 在导入过程中，Procella会尽量扫描文件中的数据。RegistrationServer只会读取文件的头来读取必要的元数据信息。但如果用户导入的文件中缺少必要的元数据信息，那么Procella会使用DS来扫描文件并生成所需的元数据信息。 实时导入在实时导入时，用户首先通过RPC或者PubSub接口将数据交给IngestionServer。 IngestionServer在收到数据之后，会将数据按照写入的表的格式进行转换，并将数据追加写到Colossus上的一个WAL日志中。这些WAL日志文件在之后会被compact成更利于查询的列存格式。 除此之外，IngestionServer还会通过双写将转换后的数据立即发送到对应的DataServer上。这些数据会被临时保存在DataServer的内存中，并立即可以对外提供查询。在正常的流程中，DataServer需要在执行查询时才会从Colossus中读取查询所需的数据。通过将新数据写入Colossus的同时就立即推送到DataServer上，Procella可以显著的提高新数据的时效性，降低查询延迟。这对于嵌入统计和监控等场景来说，是尤为关键的。 但这个时效性的提升可能会导致数据的不一致。假设用户先从DataServer的内存中查询到了新写入的数据。当DataServer出现故障丢失了内存中的这些数据，如果Colossus的写入还没有完成的话，那么用户在第二次查询时就无法访问到这些丢失的数据，产生和第一次查询时不一致的结果。为了缓解这个问题，Procella会将DataServer内存中缓存的新数据定期备份到Colossus上。这样在DataServer从故障中恢复时，丢失的新数据是有限的。但由于这个备份并不是实时的，因此备份只可以减少丢失的数据量，而无法根本避免一致性问题。如果用户对一致性要求比较高，那么就需要新数据的即时推送，要求DataServer在查询时只可以访问Colossus上已经持久化的数据。 写入WAL日志文件的数据何时可以被查询，在论文中没有介绍。这些数据可能在写入文件之后、文件关闭之后或者文件compact成列存格式之后这三种不同的情况下可以被查询。个人感觉，这些文件的数据在Compact完成之后才可以被查询。 数据维护CompactionServer会定期的对实时导入产生的WAL日志文件进行compact，生成更利于查询的列存格式的文件。在compact过程中，CompactionServer也可以执行用户自定义的合并逻辑，来对数据进行过滤或者聚合。 数据查询用户将SQL发送到RootServer来进行数据查询。RS会对SQL进解析、重写和优化，来生成执行计划。之后RS会根据数据的划分将执行计划进行拆分成物理的执行任务，将这些任务发送到对应的DS上，由这些DS来执行。当这些DS执行完成之后，由RS将最终的结果返回给用户。 在执行过程中，DS之间通过Stubby来进行shuffle。此外，如果执行计划使用Lookup的方式来进行连接，那么一个DS还可能通过RPC来访问另一个DS来按照主键检索数据。 查询优化Procella使用了动态查询优化技术来对查询进行优化。在编译阶段，优化器只会使用一个基于规则的优化器来执行一些较为明确的优化，例如谓词下推，子查询解关联等。在运行阶段，Procella会在执行过程中收集上一阶段结果的统计信息，并根据这些信息来对执行计划进行优化和调整。 为了收集运行时的统计信息，Procella当前会在每个shuffle操作之前添加一个额外的collection sites算子。由于shuffle时会对所有的数据进行遍历，collection sites算子正好可以在此时收集必要的统计信息。 基于这些收集的统计信息，Procella可以对聚合，连接和排序等算子的执行计划进行调整和优化。例如对于连接算子，Procella可以选择不同的连接方式，如广播连接，还是hash连接。 任务执行Procella的查询执行引擎，称为Superluminal，可以认为是Google之前开源的向量化引擎Supersonic的升级版。 现在很多的OLAP引擎会通过LLVM将执行计划编译成本地代码，从而提高查询的执行性能。但这些本地代码的编译本身也会带来一定的时间开销，在内嵌统计和监控这些查询吞吐很高而查询时间又很短的场景中，代码编译带来的收益就很小了。为了能够同时支持不同的数据分析场景，Superluminal使用了大量的C++模板来生成代码。这种方式可以认为是解释执行和本地执行的一种折中，一方面可以减少解释执行带来的虚函数开销，一方面又可以减少本地代码生成的编译开销。 Superluminal会充分使用向量化技术来提高数据并行度，并将数据块的大小进行调整来适合L1 Cache，提高代码执行时的Cache访问效率，尽量减少执行过程中产生的中间结果。 由于Artus格式可以在不解压数据的情况下执行很多数据操作，Superluminal会将过滤和聚合等数据操作尽量下推到Artus中执行，进一步提高数据访问效率。 任务调度在查询执行过程中，Procella也会出现长尾问题导致查询延迟变高。为了避免长尾问题，RS会监控DS执行请求的响应时间。如果一个请求的响应时间比中位数要高，那么RS会额外发送这个请求到其他DS上。 此外，RS会为每个请求附上不同的优先级。一般来说，更小的查询的优化级会更高。而DS会为不同优先级的请求维护不同的线程池。这样，更小的查询能够得到更快的响应；系统不会由于执行线程被大查询打满而导致系统整体的查询延迟和吞吐下降。 可靠性Procella的论文中没有对可靠性的细节进行详细的介绍。 可扩展性Procella依赖Borg来对计算节点进行资源管理，因而可以灵活的添加和减少DS的数目。但论文中没有介绍在扩缩容过程中，DS之间的数据划分和备份是如何动态调整的。 小结Procella的一个亮点在于导入阶段支持流批一体的导入，这对于构建一个Single Source of Truth的数据湖是尤为关键的。Procella在论文中提到了它可以支持物化视图，但也没有详细介绍物化视图是如何更新维护的。 Procella的另一个亮点是对于不同数据分析场景的支持。特别的，Procella花费了很多精力来支持内嵌统计和监控这类查询吞吐高，数据延迟和查询延迟低的查询场景。比如Procella增加了IngestionServer到DataServer的旁路来降低新数据的延迟，使用代码模板而非本地代码来避免编译开销等等。 Procella中使用Artus格式来存储数据，可以支持在压缩数据上直接进行数据的过滤和检索，也是一个非常有意思的工作。但这部分在论文中没有介绍具体的实现细节，比较遗憾。 参考文献[1] B Chattopadhyay et al. Procella: unifying serving and analytical data at Youtube. In VLDB 2020, vol. 12(12), pp. 2022-2034.","link":"/lakehouses/procella/"},{"title":"物化视图维护的代数方法","text":"为了保证与数据源的一致性，当数据源发生变化时，物化视图需要进行及时的更新。很多时候，数据源的变化相比于整体而言是较小的。此时使用增量更新的方式来对物化视图进行维护会更加高效。本文主要介绍维护物化视图的代数方法。这些方法可能在某些算子上无法提供最高效的维护方法，但在语义上比较直观。 研究历史对物化视图维护的研究最早可追溯到1980年代。这些早期的工作都局限在特定的数据库模型上。[1] 对函数式关联数据模型 (functional association data model) 上的视图维护方法进行了研究。而 [2] 只支持无环数据库 (acyclic database) 上的视图。这些视图先将数据表中所有关系表进行自然连接，之后进行映射；无法支持选择操作，也无法对部分表进行连接。 [3] 是第一个对一般物化视图的维护进行研究的工作，他们的方法可以对包含选择、映射和连接等操作在内的视图进行维护。[4] 则进一步提出使用代数方法 (algebraic method) 来描述物化视图维护的方法。在代数方法中，输入表$R$的更新被描述为一对增量表：$\\nabla R$和$\\Delta R$。其中$\\nabla R$表示$R$中被删除的记录，而$\\Delta R$表示$R$中新插入的记录。我们使用一组等式来描述这些变化是如何在算子之间传播的。在 [4] 中，作者提供了集合代数 (set algebraic) 上选择、映射、笛卡尔积、并、差、交和内连接等算子的变化传播方程 (change propagation equation)。[5] 则给出了包代数 (bag algebraic) 上相关算子的变化传播方程。 [6] 为包含外连接的视图提供了维护方法，但他们的方法并不是基于代数形式的。[7] 给出了描述维护半连接和外连接视图的代数方法。[8] 给出了基于change table方法对外连接视图进行更新的方法，但他们的方法显然是错误的。 相较于其他算子，聚合算子的变化传播很难通过代数形式进行描述。首先，聚合算子本身的代数描述就比困难。其次，部分聚合操作（如 median）并不满足分配律 (distributive property)，因此无法很难描述输出变化和输入变化之间的关系。 [5] 给出了聚合算子的变化传播方程，但他们的聚合算子无法支持分组语句 (group-by)，并且只能作为视图中最后一个算子。[8] 通过通用映射 (general projection) 来对聚合操作进行描述，给出了一般情况下可分配的聚合算子的变化传播方程。除此之外，其他在维护聚合视图上的工作 [10, 11]，都只描述了具体的过程，而没有给出形式化描述。 在维护视图的代数方法中，如果我们向后传播的插入和删除中存在重叠的记录，那么显然会导致后面的增量更新存在不必要的冗余计算。我们希望向下游传播的变化是最小化的，即$\\Delta R \\cap \\nabla R = \\emptyset$。[4] 虽然给出了常见算子的变化传播方法并号称这些方法满足最小化条件，但实际并非如此。[12] 指出了 [4] 中的错误，并给出了一个通用的构建最小化结果变化的方法。 虽然代数方法可以用于解决一般情况下的视图更新问题，但在实际中我们的视图通常会多个嵌套的算子。此时使用代数方法去逐个算子逐个输入的去计算视图的变化效率会非常低。此时，我们常常需要一些更加高效但没有那么纯粹 (pure) 的方法来计算视图的更新 [9]。 视图更新的代数方法选择 (Select)选择算子的变化传播方程如下 $$ \\sigma_p(R \\cup \\Delta R) = \\sigma_p(R) + \\sigma_p(\\Delta R)$$ $$\\sigma_p(R - \\nabla R) = \\sigma_p(R) - \\sigma_p(\\nabla R)$$ 映射 (Project)映射算子的变化传播方程如下 $$\\pi_A(R \\cup \\Delta R) = \\pi_A(R) \\cup \\pi_A(\\Delta R)$$ $$\\pi_A(R - \\nabla R) = \\pi_A(R) - \\pi_A(\\nabla R)$$ 并 (Union)集合代数和包代数下并操作有着不同的语义。我们使用UNION和UNION ALL来区分这两种不同的操作。 UNION ALL我们使用$R \\cup S$来表示表$R$和$S$在包语义下的并操作。如果一个记录$t$在$R$中出现了$n$次，在$S$中出现了$m$次。如果$n &gt; 0$或者$m&gt;0$，那么$t \\ in R \\cup S$，并且在$R\\cup S$中出现$n + m$次。 UNION ALL的变化传播方程如下 $$(R - \\nabla R) \\cup S = (R \\cup S) - \\nabla R$$ $$(R \\cup \\Delta R) \\cup S = (R \\cup S) \\cup \\Delta R$$ UNION我们使用$R\\sqcup S$表示表$R$和$S$在集合语义下的并操作。如果一个记录$t$在$R$出现了或者在$S$中出现了，那么$t$也会出现在$R \\sqcup S$中，并且只出现一次。 当$R$中某个记录$r$被删除时，对于$R \\sqcup S$可能存在以下两种情况 如果$r$在$R - \\nabla R$或者$S$中出现了，那么$r$仍然会在最终的结果中出现。$r$的删除并不会对最终的结果产生任何影响。 如果$r$既没有出现在$R - \\nabla R$中，也没有出现在$S$中，那么从$R$中删除$r$之后，$r$也会从最终的结果中被删除。我们使用$\\nabla R \\ominus ((R - \\nabla R) \\sqcup S)$ 来表示$\\nabla R$中没有出现在$R - \\nabla R$和$S$中的记录。 根据上面的讨论可知： $$ (R - \\nabla R) \\sqcup S = (R \\sqcup S) - (\\nabla R \\ominus ((R - \\nabla R) \\sqcup S))$$ 当$R$中添加某个记录$r$时，对于$R \\sqcup S$可能存在以下两种情况 如果$r$已经在$R$或者$S$中出现了，那么$r$已经在最终的结果中出现了。插入$r$并不会对最终的结果产生任何影响。 如果$r$既没有出现在$R$中，也没有出现在$S$中，那么$r$就需要添加到最终的结果中。我们使用$\\Delta R \\ominus (R \\sqcup S)$来表示$\\Delta R$中既没有出现在$R$也没有出现在$S$中的记录。 根据上面的讨论可知 $$(R \\cup \\Delta R) \\sqcup S = (R \\sqcup S) \\cup (\\Delta R \\ominus (R \\sqcup S))$$ 交 (Intersect)和并操作一样，交操作需要区分集合语义和包语义。我们使用INTERSECT ALL表示包语义下的并操作，INTERSECT表示集合语义下的并操作。 INTERSECT ALL我们使用$R \\cap S$表示表$R$和$S$在包语义下的交操作。如果一个记录$t$在$R$中出现了$n$次，在$S$中出现了$m$次，并且$n &gt; 0, m &gt; 0$，那么$t \\in R \\cap S$，并在$R \\cap S$中出现$\\min(n, m)$次。 当$R$中删除一个记录$r$时 如果$n &gt; m$，那么在删除掉$r$不会对最终的结果产生影响。 如果$n \\le m$，那么在删除掉$r$之后，应该在最终的结果中减少一次$r$的出现。 根据上面的讨论可知 $$ (R - \\nabla R) \\cap S = (R \\cap S) - (\\nabla R - (R - S)) $$ 当$R$中插入一个记录$r$时 如果$n \\ge m$，那么在添加$r$时不会对最终的结果产生影响。 如果$n &lt; m$，那么在添加$r$后，应该在最终的结果中增加一次$r$的出现。 根据上面的讨论可知 $$ (R \\cup \\Delta R) \\cap S = (R \\cap S) \\cup (\\Delta R \\cap (S - R)) $$ INTERSECT我们使用$R \\sqcap S$表示表$R$和$S$在集合语义下的交操作。如果一个记录$t$在$R$中出现了，在$S$中也出现了，那么$t \\in R \\sqcap S$，并在$R \\sqcap S$中出现一次。 参考UNION的讨论方式，可得INTERSECT的变化传播方程为 $$(R - \\nabla R) \\sqcap S = (R \\sqcap S) - (\\nabla R \\sqcap (S \\ominus (R - \\nabla R)))$$ $$(R \\cup \\Delta R) \\sqcap S = (R \\sqcap S) \\cup (\\Delta R \\sqcap (S \\ominus R)))$$ 差 (Minus)和并操作一样，差操作需要区分集合语义和包语义。我们使用MINUS ALL表示包语义下的差操作，MINUS表示集合语义下的差操作。 MINUS ALLMINUS ALL按照包语义执行差操作。我们使用$R - S$表示表$R$和$S$在包语义下的差操作。如果一个记录$t$在$R$中出现了$n$次，在$S$中出现了$m$次，并且$n &gt; m$，那么$t \\in R - S$，并且在$R - S$中出现$n - m$次。 MINUS ALL的变化传播方程如下 $$(R - \\nabla R) - S = (R - S) - \\nabla R$$ $$(R \\cup \\Delta R) - S = (R - S) \\cup \\Delta R$$ MINUSMINUS按照集合语义执行差操作。我们使用$R \\ominus S$表示表$R$和$S$在集合语义下的差操作。如果一个记录$t$在$R$中出现了，但在$S$中没有出现，那么$t \\in R \\ominus S$，并且在$R \\ominus S$只出现一次。 当我们从$R$中删除一个记录$r$时 如果$r$没有在$S$中出现，那么$r$应该会出现在原来的结果中。 如果$r$也没有在$R - \\nabla R$中出现，那么将$r$从$R$中删除之后，$r$就需要从最终的结果中删除掉。 如果$r$出现在$R - \\nabla R$中，那么将$r$从$R$中删除之后，并不会对最终的结果产生任何影响。 如果$r$出现在$S$中，那么$r$本来就不存在于原来的结果中。删除$r$并不会对最终的结果产生任何影响。 根据上面的讨论可知，当$r$既没有出现在$S$中也没有出现在$R - \\nabla R$中时，删除$r$会导致$r$从最终的结果中删除。因此 $$(R - \\nabla R) \\ominus S = (R \\ominus S) - (\\nabla R \\ominus ((R - \\nabla R) \\sqcup S))$$ 当我们向$R$中插入一个新记录$r$时 如果$r$已经出现在$S$中了，那么$r$不会出现在原来的结果中，插入$r$也不会导致最终结果的变化。 如果$r$没有在$S$中出现 如果$r$已经出现在$R$中了，那么插入$r$也不会导致最终结果的变化。 如果$r$没有出现在$R$中，那么插入$r$后$r$会出现在最终结果中。 根据上面的讨论可知，当$r$既没有出现在$S$也没有出现在$R$中时，插入$r$会导致$r$添加到最终的结果中。因此 $$(R \\cup \\Delta R) \\ominus S = (R \\ominus S) \\cup (\\Delta R \\ominus (R \\sqcup S))$$ 类似的，我们对$S$插入或删除的情况进行讨论，可得 $$ R \\ominus (S - \\nabla S) = (R \\ominus S) \\cup (\\nabla S \\sqcap (R \\ominus (S - \\nabla S)))$$ $$R \\ominus (S \\cup \\Delta S) = (R \\ominus S) - (\\Delta S \\sqcap (R \\ominus S))$$ 内连接 (Inner Join)我们用$R \\Join_p S$表示表$R$和表$S$按照连接条件$p$进行的连接操作。 内连接的变化传播方程如下 $$(R - \\nabla R) \\Join_p S = (R \\Join_p S) - (\\nabla R \\Join_p S)$$ $$(R \\cup \\Delta R) \\Join_p S = (R \\Join_p S) \\cup (\\Delta R \\Join_p S)$$ 左外连接 (Left Outer Join)我们使用$R ⟕_p S$表示表$R$和$S$按照条件$p$进行的左外连接操作。左外连接$R ⟕_p S$除了返回$R \\Join S$之外，还会将$R$中无法和$S$进行连接的记录用null补齐$S$中的字段返回。即 $$R ⟕_p S = (R \\Join_p S) \\cup ((R \\bar{\\ltimes} S) \\times {d_S})$$ 其中$d_S$ 是一个和$S$的字段一致并且所有列都为null的记录。 当$R$中某个记录$r$被删除时 如果在表$S$中存在$s$使得$p(r,s)$成立，那么$(r, s)$出现在原来的结果中，此时我们需要将$(r,s)$从最终的结果中删除。 如果在表$S$中不存在$s$使得$p(r,s)$成立，那么$(r, d_S)$ 出现在原来的结果中。此时我们需要将$(r, d_S)$从最终的结果中删除。 根据上面的讨论可知 $$(R - \\nabla R) ⟕_p S = (R ⟕_p S) - (\\nabla R ⟕_p S)$$ 当$R$中添加某个记录$r$时 如果在表$S$中存在$s$使得$p(r,s)$成立，那么$(r,s)$会出现在最终的结果中。 如果在表$S$中不存在$s$使得$p(r,s)$成立，那么$(r, d_S)$会出现在最终的结果中。 根据上面的讨论可知 $$(R \\cup \\Delta R) ⟕_p S = (R ⟕_p S) \\cup (\\Delta R ⟕_p S)$$ 类似的，我们对$S$插入或删除的情况进行讨论，可得 $$R ⟕_p (S - \\nabla S) = ((R ⟕_p S) - (R \\Join_p \\nabla S)) \\cup (((R \\ltimes_p \\nabla S) \\bar{\\ltimes}_p (S - \\nabla S)) \\times {d_S})$$ $$R ⟕_p (S \\cup \\Delta S) = ((R ⟕_p S) \\cup (R \\Join_p \\Delta S)) - (((R \\ltimes_p \\Delta S) \\bar{\\ltimes}_p S) \\times {d_S})$$ 全外连接 (Full Outer Join)我们使用$R ⟗_p S$表示表$R$和$S$按照条件$p$进行的全外连接操作。全外连接$R ⟗_p S$除了返回$R \\Join S$之外，还会将$R$中无法和$S$进行连接的记录用null补齐$S$中的字段返回，将$S$中无法和$R$进行连接的记录用null补齐$R$中的字段后返回。即 $$R ⟗_p S = (R \\Join_p S) \\cup ((R \\bar{\\ltimes} S) \\times {d_S}) \\cup ((S \\bar{\\ltimes} R) \\times {d_R})$$ 其中$d_S$ 是一个和$S$的字段一致并且所有列都为null的记录，$d_R$是一个和$R$的字段一致并且所有列都为null的记录。 和左外连接的情况类似，我们可得 $$(R - \\nabla R) ⟗_p S = ((R ⟗_p S) - (\\nabla R ⟕_p S)) \\cup (((S \\ltimes_p \\nabla R) \\bar{\\ltimes}_p (R - \\nabla R)) \\times {d_R})$$ $$(R \\cup \\Delta R) ⟗_p S = ((R ⟗_p S) \\cup (\\Delta R ⟕_p S)) - (((S \\ltimes_p \\Delta R) \\bar{\\ltimes}_p R) \\times {d_R})$$ 半连接 (Semi Join)我们使用$R \\ltimes_p S$表示表$R$和$S$按照条件$p$​进行的半连接操作。半连接$R \\ltimes_p S$返回$R$中和$S$可以按照连接条件$p$进行连接的记录，即 $$ R \\ltimes_p S = { r | r \\in R, \\exists s \\in S, p(r, s) }$$ 当$R$中某个记录$r$被删除时 如果在表$S$中存在$s$使得$p(r, s)$成立，那么$r$出现在原来的结果中。此时我们需要将$r$从最终的结果中删除。 如果在表$S$中不存在$s$使得$p(r, s)$成立，那么$r$没有出现在原来的结果中，因此也就不会对最终的结果产生影响。 根据上面的讨论可知 $$(R - \\nabla R) \\ltimes_p S = (R \\ltimes_p S) - (\\nabla R \\ltimes_p S)$$ 当$R$中添加某个记录$r$时 如果在表$S$中存在$s$使得$p(r, s)$成立，那么$r$会出现在最终的结果中。 如果在表$S$中不存在$s$使得$p(r, s)$成立，那么$r$不会出现在最终的结果中。 根据上面的讨论可知 $$(R \\cup \\Delta R) \\ltimes_p S = (R \\ltimes_p S) \\cup (\\Delta R \\ltimes_p S)$$ 类似的，我们对$S$插入或删除的情况进行讨论，可得 $$R \\ltimes_p (S - \\nabla S) = (R \\ltimes_p S) - ((R \\ltimes_p \\nabla S) \\bar{\\ltimes}_p (S - \\nabla S))$$ $$ R \\ltimes_p (S \\cup \\Delta S) = (R \\ltimes_p S) \\cup ((R \\ltimes_p \\Delta S) \\bar{\\ltimes}_p S))$$ 反半连接 (Anti Semi Join)我们使用$R \\bar{\\ltimes}_p S$表示表$R$和$S$按照条件$p$进行的反半连接操作。反半连接$R \\bar{\\ltimes}_p S$返回$R$中可以和$S$按照连接条件$p$进行连接的记录，即 $$ R \\bar{\\ltimes}_p S = { r | r \\in R, \\nexists s \\in S, p(r, s) }$$ 和半连接的情况类似，我们可得 $$(R - \\nabla R) \\bar{\\ltimes}_p S = (R \\bar{\\ltimes}_p S) - (\\nabla R \\bar{\\ltimes}_p S)$$ $$(R \\cup \\Delta R) \\bar{\\ltimes}_p S = (R \\bar{\\ltimes}_p S) \\cup (\\Delta R \\bar{\\ltimes}_p S)$$ $$R \\bar{\\ltimes}_p (S - \\nabla S) = (R \\bar{\\ltimes}_p S) \\cup ((R \\ltimes_p \\nabla S) \\bar{\\ltimes}_p (S - \\nabla S))$$ $$R \\bar{\\ltimes}_p (S \\cup \\Delta S) = (R \\bar{\\ltimes}_p S) - (R \\ltimes_p \\Delta S)$$ Related Work[1] S. Koenig et al. A transformational framework for the automatic control of derived data. In VLDB 1981, pp. 306-318. [2] O. Shmueli et al. Maintenance of views. In SIGMOD 1984, pp. 240-255. [3] J. Blakeley et al. Efficiently updating materialied views. In SIGMOD 1986, pp. 61-71. [4] X. Qian et al. Incremental recomputation of active relational expressions. In TKDE 1991 vol. 3(3) pp. 337-341. [5] T. Griffin et al. Incremental maintenance of views with duplicates. In SIGMOD 1995, pp. 328-339. [6] A. Gupta et al. Maintenance and self-maintenance of outerjoin views. In Next Generation Information Technology and Systems 1997. [7] T. Griffin et al. Algebraic change propagation for semijoin and outerjoin queries. In SIGMOD Record 1998 vol. 27(3), pp. 22-27. [8] H. Gupta et al. Incremental maintenance of aggregate and outerjoin expressions. In Information Systems 2006, vol. 31(6), pp. 435-464. [9] P. Larson et al. Efficient maintenance of materialized outer-join views. In ICDE 2007, pp. 56-65. [10] I. Mumick et al. Maintenance of data cubes and summary tables in a warehouse. In SIGMOD Record 1997, vol. 26(2), pp.100-111. [11] T. Palpanas et al. Incremental maintenance for non-distributive aggregate functions. In VLDB 2002, pp. 802-813. [12] T. Griffin et al. An improved algorithm for the incremental recomputation of active relational expressions. In TKDE 1997, vol. 9(3) pp. 508-511.","link":"/materialized-views/algebraic-methods/"},{"title":"Lossy Counting频数估计算法","text":"LossyCounting是R. Motwani和G. S. Manku在[1]中提出的另一个频数估计算法（该论文中的另一个算法为 Sticky Sampling算法）。虽然LossyCounting算法在Misra-Gries算法提出之后20年提出，但LossyCounting算法在估计误差上和Misra-Gries算法是一样的，在空间复杂度和计算复杂度上还不如Misra-Gries算法。 LossyCounting算法的对数据项的频数的估计可以满足$0 \\le f - \\hat{f} \\le \\epsilon n$，其中$f$为真实频数，$\\hat{f}$为估计频数，$n$为所有频数之和；所需的记录数为$\\frac{1}{\\epsilon}\\log(\\epsilon n)$。LossyCounting算法也可以对数据流中的频繁项进行估计，并对给定的阈值$s\\in(0,1)$满足 所有真实频数超过$sn$的数据项都能够被返回。 所有真实频数少于$(s-\\epsilon)n$的数据项都不会被返回。 算法实现LossyCounting算法将数据流划分成一个个窗口。每个窗口中包含了$\\lceil 1/\\epsilon \\rceil$个元素。每个窗口从1开始被编上号，当前窗口的编号即为$b_{current}$。如果当前已经处理的元素总数为$n$，则$b_{current}$的值为$\\left\\lfloor \\frac{n}{\\lceil 1/\\epsilon \\rceil} \\right\\rfloor \\le n\\epsilon$。 LossyCounting算法维护了一组记录$(e, f, \\Delta)$，其中$e$为数据流中的数据项，$f$为对$e$的频数的估计，而$\\Delta$则是$f$的最大可能的估计误差，也是这个元素在之前$b_{current}-1$个窗口中最多可能出现的次数。我们将这组记录记为$\\mathcal{S}$。 当一个新元素$e$到达时，我们首先检查$e$是否存在于$\\mathcal{S}$中。如果$e$已经存在于$\\mathcal{S}$中，那么我们直接将其对应的$f$加1。如果$e$没有在$\\mathcal{S}$中，那么我们在$\\mathcal{S}$中添加一个新记录$(e, 1, b_{current} - 1)$。每当我们处理完一个窗口后，我们就对$\\mathcal{S}$进行清理。对于一个记录$(e, f, \\Delta)$，如果$f + \\Delta \\le b_{current}$，那么我们就将其从$\\mathcal{S}$中删除。 当我们需要查询某个数据项$e$的频数时，如果$e$在$\\mathcal{S}$中，我们返回$f$作为其频数的估计；如果$e$不在$\\mathcal{S}$中，则返回0作为频数的估计。而当我们需要查询数据流中出现频数超过$sn$的数据项时，我们返回$\\mathcal{S}$中所有$f \\ge (s-\\epsilon)n$的数据项。 算法分析引理1. 当一个记录$(e, f, \\Delta)$被删除时，一定有$f_e \\le b_{current}$，其中$f_e$为$e$的真实频数。 证明. 我们通过归纳法证明。 当$b_{current}=1$时，即在第一个窗口时，所有记录的$f$都是和其对应真实频数相同，并且$\\Delta$都为0。如果某个记录$(e, f, \\Delta)$在第一个窗口结束时被删除，那么一定有$f_e \\le b_{current}$。 我们假设$b_{current} &lt; k$时，结论成立。 如果当$b_{current} = k$时，记录$(e, f, \\Delta)$被删除。注意到，我们在插入记录时会使用$b_{current}-1$作为$\\Delta$的值，并且在之后都不会修改$\\Delta$。因此记录$(e, f, \\Delta)$一定是在第$\\Delta + 1$个窗口中被插入到$\\mathcal{S}$中的。 而这个记录可能之前也存在于$\\mathcal{S}$中，并在某个窗口$r &lt; k$中从$\\mathcal{S}$中删除了。根据我们前面的归纳假设，当这个记录在窗口$r$中被删除时，有$f_e’ \\le r \\le \\Delta$，其中$f_e’$是数据项$e$在窗口r中被删除时的真实频数。 由于自从窗口$\\Delta +1$插入到$\\mathcal{S}$之后，$e$的每次出现都会记录到了对应的$f$中，因此有$f_e = f_e’ + f \\le \\Delta + f$。 注意到，我们在窗口$b_{current}$中只会删除$f + \\Delta \\le b_{current}$的记录，因此有$f_e \\le b_{current}$。 结论在$b_{currnet} = k$时也成立，命题得证。 定理1. 对于任意一个数据项$e$，LossyCounting算法对其频数的估计$\\hat{f}$满足 $$0 \\le f - \\hat{f} \\le \\epsilon n$$ 证明. 如果$e$不在$\\mathcal{S}$中，则$\\hat{f} = 0$。则其一定在之前某个窗口$b’ \\le b_{current}$中被删除。根据引理1可知 $$0 \\le f \\le b’ \\le b_{current} \\le \\epsilon n$$ 如果$e$在$\\mathcal{S}$中，并且$\\Delta = 0$，则说明$e$在第一个窗口就被添加到$\\mathcal{S}$中，$f = \\hat{f}$。 如果$e$在$\\mathcal{S}$中，并且$\\Delta \\ge 1$，则说明$e$在窗口$\\Delta +1$中被添加到$\\mathcal{S}$中。$e$可能在之前的某个窗口$b’$从$\\mathcal{S}$中被删除了。根据引理1可知，在删除时$e$的真实频数不超过$b’$，从而 $$0 \\le f - \\hat{f} \\le b’ \\le \\Delta \\le b_{current} \\le \\epsilon n$$ 综上，命题得证。 定理2. LossyCounting算法所保存的记录数最多为$\\frac{1}{\\epsilon}\\log(\\epsilon n)$。 证明. 令$B = b_{current}$为当前窗口的编号。对于$i \\in [1, B]$，令$d_i$表示$\\mathcal{S}$中在窗口$i$插入到$\\mathcal{S}$中的记录数目，即$\\Delta = i - 1$的记录数目。 由于我们在每个窗口结束时都会清理$\\mathcal{S}$中的记录。因此，对于$\\Delta = B-i$的记录，其数据项在窗口$B-i+1$到窗口$B$之间一定至少出现了$i$次；否则其就会在这期间从$\\mathcal{S}$中删除。 令$w = \\lceil 1/\\epsilon \\rceil$为每个窗口的大小，则我们有 $$\\sum_{i = 1}^j i \\cdot d_i \\le j \\cdot w, j = 1, 2, …, B$$ 我们根据归纳法来证明 $$\\sum_{i = 1}^j d_i \\le \\sum_{i = 1}^j \\frac{w}{i}, j = 1, 2, …, B$$ 当$j = 1$时，上式显然成立。 假设当$j &lt;k$时，上式成立。 由于$$\\begin{aligned}&amp; k\\sum_{i = 1}^k d_i \\\\=&amp; \\sum_{i = 1}^k i \\cdot d_i + \\sum_{i = 1}^1 d_i + \\sum_{i=1}^2 d_i + … + \\sum_{i = 1}^{k-1} d_i \\\\\\le &amp; kw + \\sum_{i=1}^{k-1} \\frac{(k-i)w}{i} \\\\=&amp; k\\sum_{i=1}^k \\frac{w}{i}\\end{aligned}$$因此，当$j = k$时，$\\sum_{i = 1}^j d_i \\le \\sum_{i = 1}^j \\frac{w}{i}$，结论成立。 由于$|\\mathcal{S}| = \\sum_{i = 1}^B d_i$，因此 $$|\\mathcal{S}| \\le \\sum_{i=1}^B \\frac{w}{i} \\le w \\log B \\le \\frac{1}{\\epsilon}\\log(\\epsilon n)$$ 参考文献[1] G. S. Manku, R. Motwani. Approximate Frequency Counts over Data Streams. In VLDB 2002, pp. 346-357.","link":"/sketches/lossy-counting/"},{"title":"Misra-Gries频数估计算法","text":"Misra-Gries算法最早由J. Misra和D. Gries在1982年提出 [1]。Misra-Gries算法可以看成是对Majority算法的一个扩展，可以对数据流中的频数提供相对误差为$\\epsilon$的估计，使用的空间复杂度为$\\mathrm{O}(1/\\epsilon)$。 到了2000年左右，随着对数据流研究的再次兴起，[2] 和 [3] 又重新提出了类似的算法来用于频数估计。和原始论文不同，[2] 使用了哈希表而非平衡搜索树来保证频繁项。[3] 则通过多个链表来提高清理记录时的效率，使其在最坏情况下仍然可以保证$O(1)$的开销。 原始论文侧重于频繁项的估计，并没有对频数估计的误差进行分析。[4] 证明了当$k = 1/\\epsilon$时，频数估计的相对误差为$\\epsilon$。[5] 则表明Misra-Gries算法的估计误差仅取决于那些长尾的数据项。考虑到真实环境中的数据分布通常是倾斜的（如zipf分布），这个结论可以进一步提高收紧了估计误差的边界。 [5] 进一步扩展了Misra-Gries算法，使其可以支持记录有不同权重的场景。[6] 则优化了权重更新场景下更新和合并的效率。[5] 还提供了Misra-Gries算法的合并操作，而 [7] 则对合并的空间复杂度提供了更强的边界。[7] 同时也证明了Misra-Gries算法本质上是和SpaceSaving算法等价的。 算法实现我们在Misra-Gries算法中维护了一组记录$(e, f)$，其中$e$是数据流中的数据项，$f$是对$e$的频数估计。我们将这组记录记为$\\mathcal{D}$。 Misra-Gries算法的更新算法如下所示。当一个新元素$i$到达时，我们首先检查其是否已经在$\\mathcal{D}$中存在。如果已经存在的话，那么我们直接将其对应的频数$f_i$加1；否则，我们在$\\mathcal{D}$中添加一个新记录$(e, 1)$。如果$\\mathcal{D}$中记录的数目超过了设定的阈值$k$，那么我们就需要对$\\mathcal{D}$进行清理。我们去除当前$\\mathcal{D}$中所有频数的最小值$f_m$，并将所有频数都减去$f_m$。如果一个数据项的频数变为$0$，那么我们就将其从$\\mathcal{D}$中删除。 Misra-Gries算法更新操作0if $i \\in \\mathcal{D}$, then 1 $f_i \\gets f_i + 1$ 2else 3​ $\\mathcal{D} \\gets \\mathcal{D} \\cup \\{i\\}$ 4​ $f_m = \\min \\{ f_e | e \\in \\mathcal{D} \\}$ 5​ for all $j \\in \\mathcal{D}$ 6​ $f_j \\gets f_j - f_m$ 7​ if $f_j = 0$, then 8​ $\\mathcal{D} \\gets \\mathcal{D} - \\{j \\}$ 9​ end if 10​ end for 11end if Misra-Gries算法的查询操作如下所示。当我们需要查询某个数据项的频数时，如果这个数据项存在于$\\mathcal{D}$中，我们就返回其在$\\mathcal{D}$中对应的频数；否则，我们直接返回$0$。 Misra-Gries算法查询操作0if $e \\in \\mathcal{D}$ 1​ return $f_e$ 2else 3​ return $0$ 4end if Misra-Gries算法也支持合并操作。在合并时，我们任意选择一个频繁集来作为基础，然后再将另一个频繁集中的记录逐个添加进去。Misra-Gries算法的合并操作如下所示： Misra-Gries算法合并操作0$\\mathcal{D} \\gets \\mathcal{D}_a$ 1for all $i \\in \\mathcal{D}_b$ 2​ if $i \\in \\mathcal{D}$, then 3​ $f_i \\gets f_i + f_{i, b}$ 4​ else 5​ $\\mathcal{D} \\gets \\mathcal{D} \\cup \\{i \\}$ 6​ $f_i \\gets f_{i, b}$ 7​ end if 8​ if $|\\mathcal{D}| &gt; k$, then 9​ $f_m \\gets \\min \\{ f_e | e \\in \\mathcal{D} \\}$ 10​ for all $j \\in \\mathcal{D}$ 11​ $f_j \\gets f_j - f_m$ 12​ if $f_j = 0$, then 13​ $\\mathcal{D} \\gets \\mathcal{D} - \\{j\\}$ 14​ end if 15​ end for 16​ end if 17end for 算法分析定理. Misra-Gries算法对真实频数$f$提供的估计$\\hat{f}$满足 $$ 0 \\le f - \\hat{f} \\le \\epsilon n $$ 其中$n$为所有数据项的频数之和。此时所需的空间复杂度为$\\mathrm{O}(1/\\epsilon)$。 证明. 令$n$为数据流中已经出现的记录数目，$n’$为保存在$\\mathcal{D}$中所有频数的和。 我们对于频数估计的误差来自于我们对$\\mathcal{D}$的清理操作。每次清理时，我们会将$\\mathcal{D}$中的记录都减去一个相同的值。因此在整个计算过程中，受到影响的数据项数目一定大于等于$k$。因此，我们有 $$ 0 \\le f - \\hat{f} \\le \\frac{n - n’}{k} \\le \\frac{n}{k} $$ 令$k = 1/\\epsilon$，则 $$0 \\le f - \\hat{f} \\le \\epsilon n$$ 参考文献[1] J. Misra, D. Gries. Finding repeated elements. In Science of Computer Programming 1982, vol. 2(2), pp. 143-152. [2] E. Demaine et al. Frequency estimation of internet packet streams with limited space. In ESA 2002, pp. 348-360. [3] R. Karp et al. A simple algorithm for finding frequent elements in streams and bags. In TODS 2003, vol. 28(1), pp. 51-55. [4] P. Bose et al. Bounds for frequency estimation of packet streams. In SIROCCO 2003. [5] R. Berinde et al. Space-optimal heavy hitters with strong error bounds. In PODS 2009, pp. 157-166. [6] D. Anderson et al. A high-performance algorithm for identifying frequent items in data streams. In IMC 2017, pp. 268-282. [7] P. Agarwal et al. Mergeable summaries. In PODS 2012, pp. 23-34.","link":"/sketches/misra-gries/"},{"title":"Morris频数估计算法","text":"MorrisCounter算法是R. Morris于1978年提出的一种用于估计频数的算法 [1]. 当时Morris需要编写一段代码来对大量事件进行计数，但是他能使用的只有一个8位的计数器。为了能在有限的存储空间内完成任务，他发明了MorrisCounter算法，能够使用 $ O(\\log \\log N + \\log 1/ \\epsilon + \\log 1 / \\delta ) $个比特，对频数进行估计，并且保证估计频数$\\hat{f}$和真实频数$f$之间满足： $$ Pr[|\\hat{f} - f| \\le \\epsilon f] \\ge 1 - \\delta $$ 算法实现对计数进行估计的一个简单思想就是，我们不必严格记录下每次看到的新事件。我们可以以一定的概率记录新事件。每当我们看到一个新事件，我们就抛一枚硬币。如果正面朝上，我们就增加计数；否则我们就忽略它。这种基于多次抛硬币的结果可以通过一个二项式分布来描述。通过二项式分布的标准偏差，我们就可以对估计误差进行分析。 这种抛硬币的方式虽然可以减少计数时所需的空间，但其所需的空间仍然和计数呈线性关系。为了能够进一步获得更好的空间复杂度，MorrisCounter算法对抛硬币的概率进行动态调整，随着计数的增加而逐步降低硬币朝上的概率：当第一次看到事件时，更新的频率为1，下一次为1/2，然后为1/4，以此类推。 在MorrisCounter的实现中，我们使用$c$来保存对当前计数的估计，并使用参数$b \\in (1, 2]$来作为是否计数的参数。 MorrisCounter的更新操作实现如下 MorrisCounter更新操作0randomly pick $y$ from $[0, 1]$ 1if $y &lt; b^{-c}$, then 2$~~~~c \\gets c + 1$ 3end if MorrisCounter的查询操作实现如下 MorrisCounter查询操作0return $(b^c - 1)/(b-1)$ 如果两个MorrisCounter的参数$b$是相同的，那么我们也可以将这两个MorrisCounter进行合并。当合并时，我们选择较大的计数值作为基础，将较小的计数值按照更新操作的方式累加到结果上。 MorrisCounter查询操作0$\\alpha = \\min(c_a, c_b)$, $\\beta = \\min(c_a, c_b)$ 1for $j$ in $[0, \\alpha]$ 2$~~~~$ randomly pick $y$ from $[0, 1] 3$~~~~$ if $y &lt; b^{j-\\beta}$ 4$~~~~~~~~$ $\\beta \\gets \\beta + 1$ 5$~~~~$ end if 6end for 7return $\\beta$ 算法分析令$C_n$表示经过$n$次更新操作之后$c$的值，$X_n = (b^{C_n} - 1)/(b-1)$为经过$n$次更新操作之后对真实频数的估计值。 引理1$$E[X_n] = n$$证明.$$\\begin{aligned}E[X_n] &amp;= \\sum_c Pr[C_n = c] \\frac{b^c - 1}{b - 1} \\\\&amp;= \\sum_c (Pr[C_n = c - 1]b^c + Pr[C_{n-1} = c](1 - b^{-c}))\\frac{b^c - 1}{b - 1} \\\\&amp;= \\sum_c Pr[C_{n-1} = c] \\left( b^{-c} \\frac{b^{c+1} - 1}{b-1} + (1 - b^{-c})\\frac{b^c-1}{b-1}\\right) \\\\&amp;= \\sum_c Pr[C_{n-1} = c] \\frac{b^c-1}{b-1} + 1 \\\\&amp;= E[X_{n-1}] + 1\\end{aligned}$$ 引理2.$$ var[X_n] \\le \\frac{b-1}{2} n^2$$ 证明. 令$Y_n = X_n + \\frac{1}{b-1}$，则$$\\begin{aligned}E[Y_n^2] &amp;= \\sum_c Pr[C_n = c]\\left(\\frac{b^c}{b-1}\\right)^2 \\\\&amp;= \\sum_c Pr[C_{n-1} = c] \\left( b^{-c}\\left(\\frac{b^{c+1}}{b-1}\\right)^2 + (1-b^{-c})\\left(\\frac{b^c}{b-1}\\right)^2 \\right) \\\\&amp;= \\sum_c Pr[C_{n-1} = c] \\left( \\left(\\frac{b^c}{b-1}\\right)^2 + \\frac{b^{-c}}{(b-1)^2} \\left(\\left(b^{c+1}\\right)^2 - \\left(b^c\\right)^2 \\right) \\right) \\\\&amp;= E[Y_{n-1}^2] + \\sum_c Pr[C_{n-1} = c] \\frac{b^{-c}}{(b-1)^2}(b^{c+1}-b^c)(b^{c+1}+b^c) \\\\&amp;= E[Y_{n-1}^2] + (b+1) \\sum_c Pr[C_{n-1} = c] \\frac{b^c}{b-1} \\\\&amp;= E[Y_{n-1}^2] + (b+1) E[Y_{n-1}] \\\\&amp;= E[Y_{n-1}^2] + (b+1)(n-1) \\\\&amp;= E[Y_0^2] + (b+1)\\sum_{i=0}^{n-1} i\\end{aligned}$$由于$Y_0 = \\frac{1}{b-1}$，因此$$E[X_n^2]=E\\left[\\left(Y_n - \\frac{1}{b-1}\\right)^2 \\right] \\le E[Y_n^2 - Y_0^2] = \\frac{b+1}{2}n(n-1)$$ 从而$$var[X_n] \\le E[X_n^2] - E[X_n]^2 = \\frac{b-1}{2}n^2$$ 定理. MorrisCounter对真实频数$n$的近似估计$X_n$满足 $$Pr[|X_n - n| \\le \\epsilon n] \\ge 1 - \\delta$$ 此时所需的空间复杂度为$\\mathrm{O}\\left(\\log \\frac{1}{\\epsilon} + \\log \\frac{1}{\\delta} + \\log\\log\\epsilon^2\\delta n \\right)$。 证明. 根据引理1和引理2，由Chebyshev不等式可得$$Pr[|X_n - n| \\ge \\epsilon n] \\le \\frac{b-1}{2}n^2/\\epsilon^2n^2 = \\frac{b-1}{2}\\epsilon^2$$ 令$b \\le 1 + 2\\epsilon^2\\delta$，那么我们就可以以至少$1-\\delta$的概率提供对频数相对误差为$\\epsilon$的估计。 此时MorisCounter算法所需的空间复杂度为$\\mathrm{O}\\left(\\log \\frac{1}{\\epsilon} + \\log \\frac{1}{\\delta} + \\log\\log\\epsilon^2\\delta n \\right)$。 参考文献 [1] R. Morris. Counting large numbers of events in small registers. In Communications of the ACM, vol 21, issue 10, pp. 840-842, 1978.","link":"/sketches/morris-counter/"},{"title":"Sticky Sampling频数估计算法","text":"StickySampling是R. Motwani和G. S. Manku在2002年提出的一种基于采样的频繁项估计算法 [1]。在原始论文中，作者号称它能够对数据项的频数以超过$1-\\delta$的概率提供相对误差为$\\epsilon$的估计，并返回数据流中所有频率超过给阈值的所有数据项，其所需的空间复杂度为$\\mathrm{O} \\left( \\log \\frac{1}{\\epsilon \\delta} \\right)$。但这个结论的证明可能存在问题。 算法实现在StickySampling算法中，我们维护了一组记录$(e, f)$，其中$e$为数据流中的数据项，而$f$是对$e$频数的估计。我们将这组记录记为$\\mathcal{S}$。 每当有一个新元素$e$到达时，如果其已经存在于$\\mathcal{S}$中，我们将其对应的频数$f_e$增加1；否则我们以采样率$1/r$对元素进行采样。如果$e$被采样，我们就在$\\mathcal{S}$中插入一个新记录$(e, 1)$。和MorrisCounter算法类似，StickySampling也对采样率进行动态调整。最开始的$2t$个元素，采样率为1；而对之后的$2t$个元素，$r$被设置为2，即以采样率$1/2$进行采样；再之后的$4t$个元素的采样率为$1/4$，以此类推。在这里，$t$是一个由误差概率$\\delta$和$\\epsilon$决定的一个参数。 每当采样率进行调整的时候，我们也会扫描$\\mathcal{S}$中的元素进行清理。对于每个$\\mathcal{S}$中的记录$(e, f)$，我们投掷一个无偏的硬币。如果硬币反面朝上时，我们就将这个记录的频数$f$减1。如果$f$变为0，那么就将这个记录从$\\mathcal{S}$中删除。我们重复这个投掷过程直到硬币正面朝上。在原始论文中，只提到了投掷使用的硬币是无偏的，而没有对反面朝上的概率进行明确。从后续的证明来看，硬币反面朝上的概率应为$(1-1/r)$。 当用户需要查询数据流中出现频率超过$s$的数据项时，我们就输出$\\mathcal{S}$中$f \\ge (s - \\epsilon) N$的数据项即可。 算法分析下面是原始论文中的证明。 我们按照采样率对数据流划分成一组窗口。第一个窗口包含$2t$个元素，而之后的窗口中都包含了$rt$个元素。假设我们当前窗口的采样率为$1/r$，那么之前窗口中的元素总数为$2t + 2t + 4t + … + (r/2)t = rt$。我们用$N$表示当前已经处理的元素总数，则$N = rt + rt’\\ge rt$，其中$t’ \\in [0, 1)$。从而$1/r \\ge t/N$。 对于一次衰减操作，我们减去的频数和连续投掷硬币为反面的次数一样，因此这个值服从几何分布，其超过$\\epsilon N$的概率不超过$(1 - 1/r)^{\\epsilon N}$。由于$1/r \\ge t/N$，则这个衰减值超过$\\epsilon N$的概率不超过$(1 - t/N)^{\\epsilon N}$，进而不超过$e^{-\\epsilon t}$。 在整个数据流中，频数至少为$sN(s \\in [0, 1])$的数据项最多只有$1/s$个。这些数据项中任意一个减少的频数超过$\\epsilon N$的概率最多只有$e^{-\\epsilon t}/s$。令$t \\ge \\frac{1}{\\epsilon}\\log \\left( \\frac{1}{s \\delta} \\right)$，则这些数据项有有任意一个减少的频数超过$\\epsilon N$的概率不超过$\\delta$。 在上面的证明过程中，存在以下问题 对于频数估计的误差，来自于两个方面。一个是当数据项还未在$\\mathcal{S}$由于采样导致的频数减少；以及在每次调整采样率时的衰减操作。但上面的证明过程并没有分别考虑这两方面的影响。 一个数据项可能经历多次衰减操作，并且在每次衰减操作时，$r$和$N$的值是不同的。但上面的证明过程并没有考虑多次衰减的影响。 参考文献[1] G. S. Manku, R. Motwani. Approximate Frequency Counts over Data Streams. In VLDB 2002, pp. 346-357.","link":"/sketches/sticky-sampling/"}],"tags":[{"name":"数据湖仓","slug":"数据湖仓","link":"/tags/%E6%95%B0%E6%8D%AE%E6%B9%96%E4%BB%93/"},{"name":"ETL","slug":"ETL","link":"/tags/ETL/"},{"name":"物化视图","slug":"物化视图","link":"/tags/%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE/"},{"name":"数据流","slug":"数据流","link":"/tags/%E6%95%B0%E6%8D%AE%E6%B5%81/"},{"name":"概要算法","slug":"概要算法","link":"/tags/%E6%A6%82%E8%A6%81%E7%AE%97%E6%B3%95/"},{"name":"频数估计","slug":"频数估计","link":"/tags/%E9%A2%91%E6%95%B0%E4%BC%B0%E8%AE%A1/"}],"categories":[{"name":"数据湖仓","slug":"数据湖仓","link":"/categories/%E6%95%B0%E6%8D%AE%E6%B9%96%E4%BB%93/"},{"name":"物化视图","slug":"物化视图","link":"/categories/%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE/"},{"name":"概要算法","slug":"概要算法","link":"/categories/%E6%A6%82%E8%A6%81%E7%AE%97%E6%B3%95/"},{"name":"维护方法","slug":"物化视图/维护方法","link":"/categories/%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE/%E7%BB%B4%E6%8A%A4%E6%96%B9%E6%B3%95/"},{"name":"频数估计","slug":"概要算法/频数估计","link":"/categories/%E6%A6%82%E8%A6%81%E7%AE%97%E6%B3%95/%E9%A2%91%E6%95%B0%E4%BC%B0%E8%AE%A1/"}],"pages":[{"title":"About","text":"I’m Xiaogang Shi, currently the head of the computing team at Kwaishou. Before that, I used to work as the head of the computing team at Tencent. I obtained PhD from Peking University 2016, advised by Prof. Bin Cui. I worked with Prof. Beng Chin OOI as a visiting scholar at NUS in 2013. I have served in the technical program committee of various international conferences, including KDD, SDM, DASFAA and BigData. I am also a committer of the Apache Flink project. Research InterestsDistributed and Parallel Computing, Databases, Query Optimization, Access Methods, Graph Processing, Stream Processing Publications Xiaogang Shi, Bin Cui, Gill Dobbie, Beng Chin Ooi. A unified ad-hoc data processing system. In ACM Transaction on Database Systems (TODS), vol. 42, issue 1, no. 6. Xiaogang Shi, Bin Cui, Yingxia Shao, Yunhai Tong. Tornado: A system for real-time iterative Analysis over evolving data. In Proceedings of the 2016 ACM SIGMOD International Conference on Management of Data (SIGMOD 2016), pp. 417-430. Xiaogang Shi, Bin Cui, Gill Dobbie, Beng Chin Ooi. Towards unified ad-hoc data processing. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data (SIGMOD 2014), pp. 1263-1274. Xiaogang Shi, Yanfei Lv, Yingxia Shao, Bin Cui. bCATE: A balanced contention-aware transaction execution model for highly concurrent OLTP systems. In the 14th International Conference on Web-Age Information Management (WAIM 2013), pp. 769-780.","link":"/about/index.html"}]}
{"posts":[{"title":"Lossy Counting频数估计算法","text":"LossyCounting是R. Motwani和G. S. Manku在[1]中提出的另一个频数估计算法（该论文中的另一个算法为 Sticky Sampling算法）。 LossyCounting算法的对数据项的频数的估计可以满足$0 \\le f - \\hat{f} \\le \\epsilon n$，其中$f$为真实频数，$\\hat{f}$为估计频数，$n$为所有频数之和；所需的记录数为$\\frac{1}{\\epsilon}\\log(\\epsilon n)$。LossyCounting算法也可以对数据流中的频繁项进行估计，并对给定的阈值$s\\in(0,1)$满足 所有真实频数超过$sn$的数据项都能够被返回。 所有真实频数少于$(s-\\epsilon)n$的数据项都不会被返回。 虽然LossyCounting算法在Misra-Gries算法提出之后20年提出，但LossyCounting算法在估计误差上和Misra-Gries算法是一样的，在空间复杂度和计算复杂度上还不如Misra-Gries算法。 算法实现LossyCounting算法将数据流划分成一个个窗口。每个窗口中包含了$\\lceil 1/\\epsilon \\rceil$个元素。每个窗口从1开始被编上号，当前窗口的编号即为$b_{current}$。如果当前已经处理的元素总数为$n$，则$b_{current}$的值为$\\left\\lfloor \\frac{n}{\\lceil 1/\\epsilon \\rceil} \\right\\rfloor \\le n\\epsilon$。 LossyCounting算法维护了一组记录$(e, f, \\Delta)$，其中$e$为数据流中的数据项，$f$为对$e$的频数的估计，而$\\Delta$则是$f$的最大可能的估计误差，也是这个元素在之前$b_{current}-1$个窗口中最多可能出现的次数。我们将这组记录记为$\\mathcal{S}$。 当一个新元素$e$到达时，我们首先检查$e$是否存在于$\\mathcal{S}$中。如果$e$已经存在于$\\mathcal{S}$中，那么我们直接将其对应的$f$加1。如果$e$没有在$\\mathcal{S}$中，那么我们在$\\mathcal{S}$中添加一个新记录$(e, 1, b_{current} - 1)$。每当我们处理完一个窗口后，我们就对$\\mathcal{S}$进行清理。对于一个记录$(e, f, \\Delta)$，如果$f + \\Delta \\le b_{current}$，那么我们就将其从$\\mathcal{S}$中删除。 当我们需要查询某个数据项$e$的频数时，如果$e$在$\\mathcal{S}$中，我们返回$f$作为其频数的估计；如果$e$不在$\\mathcal{S}$中，则返回0作为频数的估计。而当我们需要查询数据流中出现频数超过$sn$的数据项时，我们返回$\\mathcal{S}$中所有$f \\ge (s-\\epsilon)n$的数据项。 算法分析引理1. 当一个记录$(e, f, \\Delta)$被删除时，一定有$f_e \\le b_{current}$，其中$f_e$为$e$的真实频数。 证明. 我们通过归纳法证明。 当$b_{current}=1$时，即在第一个窗口时，所有记录的$f$都是和其对应真实频数相同，并且$\\Delta$都为0。如果某个记录$(e, f, \\Delta)$在第一个窗口结束时被删除，那么一定有$f_e \\le b_{current}$。 我们假设$b_{current} &lt; k$时，结论成立。 如果当$b_{current} = k$时，记录$(e, f, \\Delta)$被删除。注意到，我们在插入记录时会使用$b_{current}-1$作为$\\Delta$的值，并且在之后都不会修改$\\Delta$。因此记录$(e, f, \\Delta)$一定是在第$\\Delta + 1$个窗口中被插入到$\\mathcal{S}$中的。 而这个记录可能之前也存在于$\\mathcal{S}$中，并在某个窗口$r &lt; k$中从$\\mathcal{S}$中删除了。根据我们前面的归纳假设，当这个记录在窗口$r$中被删除时，有$f_e’ \\le r \\le \\Delta$，其中$f_e’$是数据项$e$在窗口r中被删除时的真实频数。 由于自从窗口$\\Delta +1$插入到$\\mathcal{S}$之后，$e$的每次出现都会记录到了对应的$f$中，因此有$f_e = f_e’ + f \\le \\Delta + f$。 注意到，我们在窗口$b_{current}$中只会删除$f + \\Delta \\le b_{current}$的记录，因此有$f_e \\le b_{current}$。 结论在$b_{currnet} = k$时也成立，命题得证。 定理1. 对于任意一个数据项$e$，LossyCounting算法对其频数的估计$\\hat{f}$满足 $$0 \\le f - \\hat{f} \\le \\epsilon n$$ 证明. 如果$e$不在$\\mathcal{S}$中，则$\\hat{f} = 0$。则其一定在之前某个窗口$b’ \\le b_{current}$中被删除。根据引理1可知 $$0 \\le f \\le b’ \\le b_{current} \\le \\epsilon n$$ 如果$e$在$\\mathcal{S}$中，并且$\\Delta = 0$，则说明$e$在第一个窗口就被添加到$\\mathcal{S}$中，$f = \\hat{f}$。 如果$e$在$\\mathcal{S}$中，并且$\\Delta \\ge 1$，则说明$e$在窗口$\\Delta +1$中被添加到$\\mathcal{S}$中。$e$可能在之前的某个窗口$b’$从$\\mathcal{S}$中被删除了。根据引理1可知，在删除时$e$的真实频数不超过$b’$，从而 $$0 \\le f - \\hat{f} \\le b’ \\le \\Delta \\le b_{current} \\le \\epsilon n$$ 综上，命题得证。 定理2. LossyCounting算法所保存的记录数最多为$\\frac{1}{\\epsilon}\\log(\\epsilon n)$。 证明. 令$B = b_{current}$为当前窗口的编号。对于$i \\in [1, B]$，令$d_i$表示$\\mathcal{S}$中在窗口$i$插入到$\\mathcal{S}$中的记录数目，即$\\Delta = i - 1$的记录数目。 由于我们在每个窗口结束时都会清理$\\mathcal{S}$中的记录。因此，对于$\\Delta = B-i$的记录，其数据项在窗口$B-i+1$到窗口$B$之间一定至少出现了$i$次；否则其就会在这期间从$\\mathcal{S}$中删除。 令$w = \\lceil 1/\\epsilon \\rceil$为每个窗口的大小，则我们有 $$\\sum_{i = 1}^j i \\cdot d_i \\le j \\cdot w, j = 1, 2, …, B$$ 我们根据归纳法来证明 $$\\sum_{i = 1}^j d_i \\le \\sum_{i = 1}^j \\frac{w}{i}, j = 1, 2, …, B$$ 当$j = 1$时，上式显然成立。 假设当$j &lt;k$时，上式成立。 由于$$\\begin{aligned}&amp; k\\sum_{i = 1}^k d_i \\\\=&amp; \\sum_{i = 1}^k i \\cdot d_i + \\sum_{i = 1}^1 d_i + \\sum_{i=1}^2 d_i + … + \\sum_{i = 1}^{k-1} d_i \\\\\\le &amp; kw + \\sum_{i=1}^{k-1} \\frac{(k-i)w}{i} \\\\=&amp; k\\sum_{i=1}^k \\frac{w}{i}\\end{aligned}$$因此，当$j = k$时，$\\sum_{i = 1}^j d_i \\le \\sum_{i = 1}^j \\frac{w}{i}$，结论成立。 由于$|\\mathcal{S}| = \\sum_{i = 1}^B d_i$，因此 $$|\\mathcal{S}| \\le \\sum_{i=1}^B \\frac{w}{i} \\le w \\log B \\le \\frac{1}{\\epsilon}\\log(\\epsilon n)$$ 参考文献[1] G. S. Manku, R. Motwani. Approximate Frequency Counts over Data Streams. In VLDB 2002, pp. 346-357.","link":"/sketches/lossy-counting/"},{"title":"Sticky Sampling频数估计算法","text":"StickySampling是R. Motwani和G. S. Manku在2002年提出的一种基于采样的频繁项估计算法 [1]。在原始论文中，作者号称它能够对数据项的频数以超过$1-\\delta$的概率提供相对误差为$\\epsilon$的估计，并返回数据流中所有频率超过给阈值的所有数据项，其所需的空间复杂度为$\\mathrm{O} \\left( \\log \\frac{1}{\\epsilon \\delta} \\right)$。但这个结论的证明可能存在问题。 算法实现在StickySampling算法中，我们维护了一组记录$(e, f)$，其中$e$为数据流中的数据项，而$f$是对$e$频数的估计。我们将这组记录记为$\\mathcal{S}$。 每当有一个新元素$e$到达时，如果其已经存在于$\\mathcal{S}$中，我们将其对应的频数$f_e$增加1；否则我们以采样率$1/r$对元素进行采样。如果$e$被采样，我们就在$\\mathcal{S}$中插入一个新记录$(e, 1)$。和MorrisCounter算法类似，StickySampling也对采样率进行动态调整。最开始的$2t$个元素，采样率为1；而对之后的$2t$个元素，$r$被设置为2，即以采样率$1/2$进行采样；再之后的$4t$个元素的采样率为$1/4$，以此类推。在这里，$t$是一个由误差概率$\\delta$和$\\epsilon$决定的一个参数。 每当采样率进行调整的时候，我们也会扫描$\\mathcal{S}$中的元素进行清理。对于每个$\\mathcal{S}$中的记录$(e, f)$，我们投掷一个无偏的硬币。如果硬币反面朝上时，我们就将这个记录的频数$f$减1。如果$f$变为0，那么就将这个记录从$\\mathcal{S}$中删除。我们重复这个投掷过程直到硬币正面朝上。在原始论文中，只提到了投掷使用的硬币是无偏的，而没有对反面朝上的概率进行明确。从后续的证明来看，硬币反面朝上的概率应为$(1-1/r)$。 当用户需要查询数据流中出现频率超过$s$的数据项时，我们就输出$\\mathcal{S}$中$f \\ge (s - \\epsilon) N$的数据项即可。 算法分析下面是原始论文中的证明。 我们按照采样率对数据流划分成一组窗口。第一个窗口包含$2t$个元素，而之后的窗口中都包含了$rt$个元素。假设我们当前窗口的采样率为$1/r$，那么之前窗口中的元素总数为$2t + 2t + 4t + … + (r/2)t = rt$。我们用$N$表示当前已经处理的元素总数，则$N = rt + rt’\\ge rt$，其中$t’ \\in [0, 1)$。从而$1/r \\ge t/N$。 对于一次衰减操作，我们减去的频数和连续投掷硬币为反面的次数一样，因此这个值服从几何分布，其超过$\\epsilon N$的概率不超过$(1 - 1/r)^{\\epsilon N}$。由于$1/r \\ge t/N$，则这个衰减值超过$\\epsilon N$的概率不超过$(1 - t/N)^{\\epsilon N}$，进而不超过$e^{-\\epsilon t}$。 在整个数据流中，频数至少为$sN(s \\in [0, 1])$的数据项最多只有$1/s$个。这些数据项中任意一个减少的频数超过$\\epsilon N$的概率最多只有$e^{-\\epsilon t}/s$。令$t \\ge \\frac{1}{\\epsilon}\\log \\left( \\frac{1}{s \\delta} \\right)$，则这些数据项有有任意一个减少的频数超过$\\epsilon N$的概率不超过$\\delta$。 在上面的证明过程中，存在以下问题 对于频数估计的误差，来自于两个方面。一个是当数据项还未在$\\mathcal{S}$由于采样导致的频数减少；以及在每次调整采样率时的衰减操作。但上面的证明过程并没有分别考虑这两方面的影响。 一个数据项可能经历多次衰减操作，并且在每次衰减操作时，$r$和$N$的值是不同的。但上面的证明过程并没有考虑多次衰减的影响。 参考文献[1] G. S. Manku, R. Motwani. Approximate Frequency Counts over Data Streams. In VLDB 2002, pp. 346-357.","link":"/sketches/sticky-sampling/"},{"title":"Morris频数估计算法","text":"MorrisCounter算法是R. Morris于1978年提出的一种用于估计频数的算法 [1]. 当时Morris需要编写一段代码来对大量事件进行计数，但是他能使用的只有一个8位的计数器。为了能在有限的存储空间内完成任务，他发明了MorrisCounter算法，能够使用 $ O(\\log \\log N + \\log 1/ \\epsilon + \\log 1 / \\delta ) $个比特，对频数进行估计，并且保证估计频数$\\hat{f}$和真实频数$f$之间满足： $$ Pr[|\\hat{f} - f| \\le \\epsilon f] \\ge 1 - \\delta $$ 算法实现对计数进行估计的一个简单思想就是，我们不必严格记录下每次看到的新事件。我们可以以一定的概率记录新事件。每当我们看到一个新事件，我们就抛一枚硬币。如果正面朝上，我们就增加计数；否则我们就忽略它。这种基于多次抛硬币的结果可以通过一个二项式分布来描述。通过二项式分布的标准偏差，我们就可以对估计误差进行分析。 这种抛硬币的方式虽然可以减少计数时所需的空间，但其所需的空间仍然和计数呈线性关系。为了能够进一步获得更好的空间复杂度，MorrisCounter算法对抛硬币的概率进行动态调整，随着计数的增加而逐步降低硬币朝上的概率：当第一次看到事件时，更新的频率为1，下一次为1/2，然后为1/4，以此类推。 在MorrisCounter的实现中，我们使用$c$来保存对当前计数的估计，并使用参数$b \\in (1, 2]$来作为是否计数的参数。 MorrisCounter的更新操作实现如下 MorrisCounter更新操作0randomly pick $y$ from $[0, 1]$ 1if $y &lt; b^{-c}$, then 2$~~~~c \\gets c + 1$ 3end if MorrisCounter的查询操作实现如下 MorrisCounter查询操作0return $(b^c - 1)/(b-1)$ 如果两个MorrisCounter的参数$b$是相同的，那么我们也可以将这两个MorrisCounter进行合并。当合并时，我们选择较大的计数值作为基础，将较小的计数值按照更新操作的方式累加到结果上。 MorrisCounter查询操作0$\\alpha = \\min(c_a, c_b)$, $\\beta = \\min(c_a, c_b)$ 1for $j$ in $[0, \\alpha]$ 2$~~~~$ randomly pick $y$ from $[0, 1] 3$~~~~$ if $y &lt; b^{j-\\beta}$ 4$~~~~~~~~$ $\\beta \\gets \\beta + 1$ 5$~~~~$ end if 6end for 7return $\\beta$ 算法分析令$C_n$表示经过$n$次更新操作之后$c$的值，$X_n = (b^{C_n} - 1)/(b-1)$为经过$n$次更新操作之后对真实频数的估计值。 引理1$$E[X_n] = n$$证明.$$\\begin{aligned}E[X_n] &amp;= \\sum_c Pr[C_n = c] \\frac{b^c - 1}{b - 1} \\\\&amp;= \\sum_c (Pr[C_n = c - 1]b^c + Pr[C_{n-1} = c](1 - b^{-c}))\\frac{b^c - 1}{b - 1} \\\\&amp;= \\sum_c Pr[C_{n-1} = c] \\left( b^{-c} \\frac{b^{c+1} - 1}{b-1} + (1 - b^{-c})\\frac{b^c-1}{b-1}\\right) \\\\&amp;= \\sum_c Pr[C_{n-1} = c] \\frac{b^c-1}{b-1} + 1 \\\\&amp;= E[X_{n-1}] + 1\\end{aligned}$$ 引理2.$$ var[X_n] \\le \\frac{b-1}{2} n^2$$ 证明. 令$Y_n = X_n + \\frac{1}{b-1}$，则$$\\begin{aligned}E[Y_n^2] &amp;= \\sum_c Pr[C_n = c]\\left(\\frac{b^c}{b-1}\\right)^2 \\\\&amp;= \\sum_c Pr[C_{n-1} = c] \\left( b^{-c}\\left(\\frac{b^{c+1}}{b-1}\\right)^2 + (1-b^{-c})\\left(\\frac{b^c}{b-1}\\right)^2 \\right) \\\\&amp;= \\sum_c Pr[C_{n-1} = c] \\left( \\left(\\frac{b^c}{b-1}\\right)^2 + \\frac{b^{-c}}{(b-1)^2} \\left(\\left(b^{c+1}\\right)^2 - \\left(b^c\\right)^2 \\right) \\right) \\\\&amp;= E[Y_{n-1}^2] + \\sum_c Pr[C_{n-1} = c] \\frac{b^{-c}}{(b-1)^2}(b^{c+1}-b^c)(b^{c+1}+b^c) \\\\&amp;= E[Y_{n-1}^2] + (b+1) \\sum_c Pr[C_{n-1} = c] \\frac{b^c}{b-1} \\\\&amp;= E[Y_{n-1}^2] + (b+1) E[Y_{n-1}] \\\\&amp;= E[Y_{n-1}^2] + (b+1)(n-1) \\\\&amp;= E[Y_0^2] + (b+1)\\sum_{i=0}^{n-1} i\\end{aligned}$$由于$Y_0 = \\frac{1}{b-1}$，因此$$E[X_n^2]=E\\left[\\left(Y_n - \\frac{1}{b-1}\\right)^2 \\right] \\le E[Y_n^2 - Y_0^2] = \\frac{b+1}{2}n(n-1)$$ 从而$$var[X_n] \\le E[X_n^2] - E[X_n]^2 = \\frac{b-1}{2}n^2$$ 定理. MorrisCounter对真实频数$n$的近似估计$X_n$满足 $$Pr[|X_n - n| \\le \\epsilon n] \\ge 1 - \\delta$$ 此时所需的空间复杂度为$\\mathrm{O}\\left(\\log \\frac{1}{\\epsilon} + \\log \\frac{1}{\\delta} + \\log\\log\\epsilon^2\\delta n \\right)$。 证明. 根据引理1和引理2，由Chebyshev不等式可得$$Pr[|X_n - n| \\ge \\epsilon n] \\le \\frac{b-1}{2}n^2/\\epsilon^2n^2 = \\frac{b-1}{2}\\epsilon^2$$ 令$b \\le 1 + 2\\epsilon^2\\delta$，那么我们就可以以至少$1-\\delta$的概率提供对频数相对误差为$\\epsilon$的估计。 此时MorisCounter算法所需的空间复杂度为$\\mathrm{O}\\left(\\log \\frac{1}{\\epsilon} + \\log \\frac{1}{\\delta} + \\log\\log\\epsilon^2\\delta n \\right)$。 参考文献 [1] R. Morris. Counting large numbers of events in small registers. In Communications of the ACM, vol 21, issue 10, pp. 840-842, 1978.","link":"/sketches/morris-counter/"}],"tags":[{"name":"数据流","slug":"数据流","link":"/tags/%E6%95%B0%E6%8D%AE%E6%B5%81/"},{"name":"概要算法","slug":"概要算法","link":"/tags/%E6%A6%82%E8%A6%81%E7%AE%97%E6%B3%95/"},{"name":"频数估计","slug":"频数估计","link":"/tags/%E9%A2%91%E6%95%B0%E4%BC%B0%E8%AE%A1/"}],"categories":[{"name":"概要算法","slug":"概要算法","link":"/categories/%E6%A6%82%E8%A6%81%E7%AE%97%E6%B3%95/"},{"name":"频数估计","slug":"概要算法/频数估计","link":"/categories/%E6%A6%82%E8%A6%81%E7%AE%97%E6%B3%95/%E9%A2%91%E6%95%B0%E4%BC%B0%E8%AE%A1/"}],"pages":[{"title":"About","text":"I’m Xiaogang Shi, currently the head of the computing team at Kwaishou. Before that, I used to work as the head of the computing team at Tencent. I obtained PhD from Peking University 2016, advised by Prof. Bin Cui. I worked with Prof. Beng Chin OOI as a visiting scholar at NUS in 2013. I have served in the technical program committee of various international conferences, including KDD, SDM, DASFAA and BigData. I am also a committer of the Apache Flink project. Research InterestsDistributed and Parallel Computing, Databases, Query Optimization, Access Methods, Graph Processing, Stream Processing Publications Xiaogang Shi, Bin Cui, Gill Dobbie, Beng Chin Ooi. A unified ad-hoc data processing system. In ACM Transaction on Database Systems (TODS), vol. 42, issue 1, no. 6. Xiaogang Shi, Bin Cui, Yingxia Shao, Yunhai Tong. Tornado: A system for real-time iterative Analysis over evolving data. In Proceedings of the 2016 ACM SIGMOD International Conference on Management of Data (SIGMOD 2016), pp. 417-430. Xiaogang Shi, Bin Cui, Gill Dobbie, Beng Chin Ooi. Towards unified ad-hoc data processing. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data (SIGMOD 2014), pp. 1263-1274. Xiaogang Shi, Yanfei Lv, Yingxia Shao, Bin Cui. bCATE: A balanced contention-aware transaction execution model for highly concurrent OLTP systems. In the 14th International Conference on Web-Age Information Management (WAIM 2013), pp. 769-780.","link":"/about/index.html"}]}